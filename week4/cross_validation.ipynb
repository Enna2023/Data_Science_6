{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "Our dataset should be as large as possible to train the model and removing considerable part of it for validation poses a problem of losing valuable portion of data that we would prefer to be able to train.\n",
    "\n",
    "In order to address this issue, we use the Cross validation technique. Cross Validation has a number of types out of which we'll be using K-fold cross validation today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the column names are numerical, we will give our own column names for our understanding\n",
    "col = [\"num_preg\", \"plasma_glucose_conc\", \"D_blood_pressure\", \"skin_fold_thickness\", \"serum_insulin\", \"body_mass_index\", \"pedigree_func\", \"age\", \"diabetes\"]\n",
    "diabetes_data = pd.read_csv(\"https://raw.githubusercontent.com/dphi-official/ML_Models/master/Performance_Evaluation/diabetes.txt\", names = col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the diabetes data is use\n",
    "# separate input and output vairables\n",
    "X = diabetes_data.drop('diabetes', axis = 1)\n",
    "y = diabetes_data.diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "mlp.fit(x_train, y_train)\n",
    "y_pred = mlp.predict(x_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.31637096, 0.24822402, 0.28818059, 0.26880527, 0.35934663,\n",
       "        0.21173573, 0.19422269, 0.29083443, 0.16726637, 0.46691942]),\n",
       " 'score_time': array([0.00500631, 0.00400352, 0.00400186, 0.00400305, 0.00400352,\n",
       "        0.00300217, 0.00300121, 0.00300217, 0.00462794, 0.00400352]),\n",
       " 'test_accuracy': array([0.67532468, 0.67532468, 0.67532468, 0.75324675, 0.66233766,\n",
       "        0.75324675, 0.74025974, 0.64935065, 0.65789474, 0.64473684]),\n",
       " 'test_precision': array([0.53846154, 0.5625    , 0.54166667, 0.66666667, 0.53333333,\n",
       "        0.75      , 0.76923077, 0.5       , 0.5       , 0.48648649]),\n",
       " 'test_recall': array([0.51851852, 0.33333333, 0.48148148, 0.59259259, 0.2962963 ,\n",
       "        0.44444444, 0.37037037, 0.44444444, 0.15384615, 0.69230769])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = cross_validate(mlp, X, y, cv=10, scoring=[\"accuracy\", \"precision\", \"recall\"])\n",
    "cv_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv=10 is provided, which means we are performing 10 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6887047163362953\n",
      "Precision:  0.5848345460845461\n",
      "Recall:  0.4327635327635327\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", cv_results[\"test_accuracy\"].mean())\n",
    "print(\"Precision: \", cv_results[\"test_precision\"].mean())\n",
    "print(\"Recall: \", cv_results[\"test_recall\"].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all valid scoring options - use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['explained_variance', 'r2', 'max_error', 'matthews_corrcoef', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'top_k_accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'positive_likelihood_ratio', 'neg_negative_likelihood_ratio', 'adjusted_rand_score', 'rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.metrics as m\n",
    "m.SCORERS.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complicated scoring metrics (such as specificity, which isn't explicilty provided by sklearn), or to create your own metrics, http://scikit-learn.org/stable/modules/model_evaluation.html#using-multiple-metric-evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave One Out Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.21519494, 0.17018461, 0.21847749, 0.53748822, 0.23120999,\n",
       "        0.19817948, 0.29526687, 0.16514993, 0.18216562, 0.31028152,\n",
       "        0.25129485, 0.16815281, 0.23621464, 0.30627799, 0.3413105 ,\n",
       "        0.26724243, 0.3333652 , 0.43189836, 0.3152864 , 0.20828485,\n",
       "        0.44039989, 0.18316603, 0.12411237, 0.4343946 , 0.33236432,\n",
       "        0.37834382, 0.25222898, 0.18716908, 0.26524115, 0.28125501,\n",
       "        0.48443961, 0.43939877, 0.42438745, 0.19317341, 0.33630514,\n",
       "        0.30827999, 0.25723386, 0.21119165, 0.44940782, 0.14613271,\n",
       "        0.2332809 , 0.29326463, 0.26323938, 0.31828904, 0.47643232,\n",
       "        0.21519566, 0.38234735, 0.25923705, 0.31979442, 0.53548574,\n",
       "        0.30127358, 0.3353045 , 0.33029962, 0.19117427, 0.40837121,\n",
       "        0.27331185, 0.50445795, 0.26524186, 0.27925467, 0.22128153,\n",
       "        0.30727911, 0.36432958, 0.26123762, 0.4103713 , 0.24422169,\n",
       "        0.29126453, 0.18616891, 0.13412166, 0.37834382, 0.30527711,\n",
       "        0.24121928, 0.2362144 , 0.1991806 , 0.19417667, 0.4924469 ,\n",
       "        0.19217467, 0.24470186, 0.22520471, 0.35832453, 0.17020941,\n",
       "        0.35031819, 0.46742439, 0.22320247, 0.21791196, 0.32129622,\n",
       "        0.24822474, 0.25273395, 0.21519518, 0.30933928, 0.41837978,\n",
       "        0.28826213, 0.26123667, 0.36433101, 0.25122809, 0.27875829,\n",
       "        0.42238355, 0.2552309 , 0.21919894, 0.44039989, 0.24822593,\n",
       "        0.42138219, 0.32029057, 0.4204433 , 0.27524972, 0.25423145,\n",
       "        0.19417691, 0.30427623, 0.18116403, 0.28325653, 0.18616915,\n",
       "        0.41246533, 0.28225636, 0.36232924, 0.40336633, 0.25222921,\n",
       "        0.30827928, 0.25834894, 0.3002727 , 0.31779766, 0.20518637,\n",
       "        0.55550408, 0.4764328 , 0.28025484, 0.36733365, 0.27424836,\n",
       "        0.19924641, 0.30327559, 0.23521376, 0.28125501, 0.23120952,\n",
       "        0.19717836, 0.10009074, 0.2912643 , 0.19617796, 0.21519518,\n",
       "        0.19517756, 0.35031772, 0.24922633, 0.26424026, 0.33030009,\n",
       "        0.20618725, 0.22820711, 0.32929873, 0.27524972, 0.28425813,\n",
       "        0.23321176, 0.18316603, 0.39135528, 0.15313935, 0.28325725,\n",
       "        0.25022721, 0.21019101, 0.34831643, 0.35231996, 0.16314793,\n",
       "        0.33930802, 0.31528544, 0.31728792, 0.29426718, 0.40343046,\n",
       "        0.39335704, 0.17515993, 0.49444842, 0.30827975, 0.31328464,\n",
       "        0.35732436, 0.46942616, 0.41437602, 0.24722409, 0.14813447,\n",
       "        0.48343873, 0.40937185, 0.33430338, 0.33030009, 0.33637857,\n",
       "        0.29727006, 0.16014552, 0.37241077, 0.24522305, 0.42838883,\n",
       "        0.27224636, 0.38534999, 0.37433863, 0.2782526 , 0.17916274,\n",
       "        0.20818901, 0.25923538, 0.49795747, 0.2732482 , 0.29927206,\n",
       "        0.20018148, 0.39936256, 0.41837978, 0.19317579, 0.5056572 ,\n",
       "        0.37584686, 0.27525067, 0.44740629, 0.37640095, 0.40736985,\n",
       "        0.3022747 , 0.38034558, 0.17115498, 0.18516827, 0.33930802,\n",
       "        0.19117332, 0.24121928, 0.29126525, 0.3733387 , 0.16414857,\n",
       "        0.37233829, 0.30727863, 0.23421311, 0.28625989, 0.2573359 ,\n",
       "        0.25022674, 0.418437  , 0.29727054, 0.41537738, 0.18717027,\n",
       "        0.49845147, 0.21619606, 0.31628704, 0.2732482 , 0.3022747 ,\n",
       "        0.28926229, 0.28025436, 0.3413105 , 0.13812566, 0.40837049,\n",
       "        0.22620583, 0.32536387, 0.30627823, 0.23221087, 0.27124667,\n",
       "        0.12211108, 0.29076958, 0.30928183, 0.40036297, 0.32629657,\n",
       "        0.36533165, 0.15514112, 0.33430362, 0.43947482, 0.50746083,\n",
       "        0.31734657, 0.36132789, 0.35031891, 0.23071575, 0.29827094,\n",
       "        0.275249  , 0.22920799, 0.32841039, 0.30627823, 0.18817067,\n",
       "        0.40736985, 0.24822426, 0.31728888, 0.12211061, 0.29126453,\n",
       "        0.20618677, 0.3313005 , 0.25523067, 0.25022674, 0.36232829,\n",
       "        0.36633277, 0.26323795, 0.39535904, 0.27796483, 0.27875733,\n",
       "        0.36733437, 0.47342992, 0.28635597, 0.28726053, 0.26123738,\n",
       "        0.32129169, 0.24422145, 0.23171663, 0.50545859, 0.2762506 ,\n",
       "        0.30833817, 0.34931684, 0.43339372, 0.1811645 , 0.18616939,\n",
       "        0.30327511, 0.18216538, 0.30827975, 0.29526806, 0.16920853,\n",
       "        0.39135599, 0.27925372, 0.39635992, 0.2161963 , 0.21819854,\n",
       "        0.40736914, 0.39844966, 0.39035463, 0.33787632, 0.50045609,\n",
       "        0.35432148, 0.22420359, 0.23421288, 0.23120999, 0.21931434,\n",
       "        0.34331179, 0.36833429, 0.21827102, 0.2212007 , 0.47655249,\n",
       "        0.35039306, 0.27324748, 0.22620535, 0.42649531, 0.2722466 ,\n",
       "        0.28626013, 0.15514112, 0.2011826 , 0.40636873, 0.3257854 ,\n",
       "        0.50516176, 0.18516827, 0.20330787, 0.21219206, 0.21919894,\n",
       "        0.51747084, 0.40937257, 0.21819758, 0.19117403, 0.32237434,\n",
       "        0.6335752 , 0.28025413, 0.44210267, 0.28525853, 0.17515492,\n",
       "        0.40937209, 0.38935328, 0.38634992, 0.14713335, 0.46542192,\n",
       "        0.13312054, 0.52147365, 0.40436697, 0.26724267, 0.2242043 ,\n",
       "        0.3838551 , 0.16314864, 0.16214871, 0.31033683, 0.40236568,\n",
       "        0.14613271, 0.3473146 , 0.31542253, 0.49653816, 0.3403089 ,\n",
       "        0.45741558, 0.37534022, 0.28826118, 0.32129169, 0.43239236,\n",
       "        0.30027223, 0.23821616, 0.23221111, 0.24221992, 0.17415953,\n",
       "        0.24321699, 0.22720623, 0.29626918, 0.22420359, 0.21719718,\n",
       "        0.31928849, 0.25022721, 0.3222928 , 0.24922633, 0.22420382,\n",
       "        0.48043633, 0.32179737, 0.15313888, 0.17315555, 0.36341691,\n",
       "        0.26724267, 0.27725148, 0.21519542, 0.44039965, 0.21119189,\n",
       "        0.22620559, 0.52347493, 0.35231996, 0.3403089 , 0.48048401,\n",
       "        0.18516779, 0.36433101, 0.21019101, 0.1991806 , 0.30327535,\n",
       "        0.32735324, 0.43439364, 0.40837169, 0.52747798, 0.21519542,\n",
       "        0.45140982, 0.2929709 , 0.45541382, 0.23028207, 0.31028104,\n",
       "        0.25122809, 0.16815186, 0.23071504, 0.40636921, 0.29927158,\n",
       "        0.2602365 , 0.52747869, 0.31528687, 0.44846749, 0.42438507,\n",
       "        0.34831572, 0.47753263, 0.22520447, 0.19417572, 0.32029104,\n",
       "        0.23821616, 0.45341182, 0.42838883, 0.23221111, 0.11210203,\n",
       "        0.47843432, 0.14413071, 0.34431171, 0.35231996, 0.29827023,\n",
       "        0.43739748, 0.34131026, 0.19417572, 0.40636897, 0.46241951,\n",
       "        0.24221992, 0.32035065, 0.42038202, 0.39936233, 0.43144989,\n",
       "        0.23921728, 0.13712382, 0.41537738, 0.14413071, 0.30478215,\n",
       "        0.29326653, 0.28133178, 0.40636945, 0.28826165, 0.29927135,\n",
       "        0.32029009, 0.28431368, 0.35031819, 0.22020006, 0.31428599,\n",
       "        0.35883093, 0.42945623, 0.51146531, 0.33530259, 0.59253788,\n",
       "        0.25923634, 0.28525853, 0.2912643 , 0.36933541, 0.18626952,\n",
       "        0.35632277, 0.25423098, 0.12711549, 0.38635063, 0.4554131 ,\n",
       "        0.28826189, 0.18016386, 0.33130169, 0.2161963 , 0.4374609 ,\n",
       "        0.34636855, 0.1921742 , 0.39235592, 0.19017291, 0.52654147,\n",
       "        0.48544049, 0.27825308, 0.64964604, 0.29326558, 0.31128407,\n",
       "        0.3092804 , 0.16715169, 0.17115593, 0.57159233, 0.32880402,\n",
       "        0.21425128, 0.27024674, 0.21519518, 0.16915369, 0.17115498,\n",
       "        0.24121952, 0.14919138, 0.45040894, 0.1611464 , 0.33230138,\n",
       "        0.33630538, 0.27324843, 0.46141887, 0.29927158, 0.55650687,\n",
       "        0.24722362, 0.1481359 , 0.30327272, 0.20718813, 0.33630514,\n",
       "        0.25723338, 0.44940758, 0.20818925, 0.31638265, 0.11810756,\n",
       "        0.2141943 , 0.16615176, 0.2161963 , 0.44740605, 0.35983467,\n",
       "        0.32329345, 0.13719463, 0.27224684, 0.22820735, 0.44711399,\n",
       "        0.27925348, 0.22820711, 0.35839176, 0.2932663 , 0.16014528,\n",
       "        0.20718789, 0.19824219, 0.43239212, 0.2151947 , 0.20518637,\n",
       "        0.59059501, 0.24021721, 0.22920799, 0.39735985, 0.30827951,\n",
       "        0.26123738, 0.31928992, 0.42538595, 0.54849792, 0.10809755,\n",
       "        0.14012718, 0.19825673, 0.32329345, 0.41637826, 0.21325183,\n",
       "        0.33530521, 0.11310315, 0.35932589, 0.53098774, 0.39736104,\n",
       "        0.2833581 , 0.27224731, 0.24322104, 0.23921824, 0.40845728,\n",
       "        0.19217467, 0.25723338, 0.21419454, 0.43846965, 0.32129216,\n",
       "        0.21219277, 0.34531379, 0.36140561, 0.39235616, 0.36232901,\n",
       "        0.18923759, 0.31628728, 0.30627751, 0.2181983 , 0.18523622,\n",
       "        0.40859175, 0.19317508, 0.43539524, 0.40036392, 0.31234241,\n",
       "        0.35031867, 0.42138243, 0.51146412, 0.27725124, 0.35932732,\n",
       "        0.28325558, 0.14118505, 0.28625989, 0.29533696, 0.26624203,\n",
       "        0.23021078, 0.352319  , 0.16220856, 0.25723362, 0.29126382,\n",
       "        0.27825212, 0.29626846, 0.23521376, 0.26724267, 0.21219254,\n",
       "        0.37433934, 0.42838883, 0.1781621 , 0.28926206, 0.32029033,\n",
       "        0.29032183, 0.28025413, 0.28694487, 0.27424932, 0.53948975,\n",
       "        0.28525972, 0.50646043, 0.40036297, 0.31328464, 0.25523186,\n",
       "        0.27731061, 0.43239284, 0.21975684, 0.37333894, 0.3092804 ,\n",
       "        0.30527663, 0.25594854, 0.12411261, 0.13212013, 0.19517756,\n",
       "        0.31328487, 0.45851636, 0.17816162, 0.35732484, 0.25022745,\n",
       "        0.34431267, 0.15213752, 0.32529521, 0.19617748, 0.27725196,\n",
       "        0.14413071, 0.15514183, 0.40408206, 0.34931731, 0.27224636,\n",
       "        0.35031986, 0.40787625, 0.36232853, 0.37534046, 0.23221064,\n",
       "        0.19317532, 0.39635968, 0.31137681, 0.28425884, 0.42338443,\n",
       "        0.24121809, 0.24322081, 0.39786625, 0.30727887, 0.18917179,\n",
       "        0.1611464 , 0.26624179, 0.24822569, 0.33130074, 0.3222928 ,\n",
       "        0.29626918, 0.2752502 , 0.49101591, 0.37534094, 0.18516803,\n",
       "        0.42338419, 0.31828904, 0.2222023 , 0.18416834, 0.31328344,\n",
       "        0.30027175, 0.33636069, 0.27931142, 0.23121047, 0.3217988 ,\n",
       "        0.24522305, 0.28726101, 0.30834723, 0.3142848 , 0.35332036,\n",
       "        0.23321152, 0.42438459, 0.30728126, 0.28726149, 0.30627823,\n",
       "        0.34931731, 0.38334751, 0.17015433, 0.32529569, 0.27825236,\n",
       "        0.20918989, 0.26724267, 0.5992434 , 0.21119261, 0.20718694,\n",
       "        0.4704268 , 0.38334799, 0.26123667, 0.44740629, 0.49495482,\n",
       "        0.14521575, 0.29026294, 0.32935882, 0.18724489, 0.4504087 ,\n",
       "        0.27725148, 0.22620511, 0.35331941, 0.39135528, 0.35632324,\n",
       "        0.56351113, 0.37133718, 0.45240998, 0.3272965 , 0.21519566,\n",
       "        0.21525693, 0.18917227, 0.33288455, 0.33130097, 0.41337657,\n",
       "        0.34030771, 0.45741558, 0.33839869, 0.34130907, 0.37033653,\n",
       "        0.27925301, 0.3793447 , 0.27524996, 0.23921728, 0.71364784,\n",
       "        0.28926277, 0.29126477, 0.4173789 , 0.27725172, 0.439399  ,\n",
       "        0.38635111, 0.34130931, 0.23020911, 0.45841646, 0.53548574,\n",
       "        0.1311183 , 0.40343356, 0.23321176, 0.62957048, 0.39936256,\n",
       "        0.37333918, 0.35332108, 0.22220182, 0.19217443, 0.20518684,\n",
       "        0.33129954, 0.34087658, 0.12711549, 0.33930826, 0.40542865,\n",
       "        0.21619582, 0.18216562, 0.33430362, 0.45341158, 0.36842704,\n",
       "        0.4263854 , 0.23921728, 0.39335728, 0.28726006, 0.20518637,\n",
       "        0.26929975, 0.431391  , 0.26023602]),\n",
       " 'score_time': array([0.00200176, 0.00200248, 0.00200176, 0.00200129, 0.002002  ,\n",
       "        0.00100064, 0.00100088, 0.00100112, 0.00200176, 0.00100064,\n",
       "        0.00100064, 0.00100088, 0.00200152, 0.00100064, 0.00200129,\n",
       "        0.00100112, 0.00100064, 0.00200176, 0.0010004 , 0.00100183,\n",
       "        0.00100088, 0.00100088, 0.00100088, 0.00200129, 0.00100064,\n",
       "        0.00100064, 0.00100183, 0.00200176, 0.00100064, 0.00100112,\n",
       "        0.0010016 , 0.00100064, 0.00200152, 0.00100088, 0.00100136,\n",
       "        0.00200176, 0.00200152, 0.00100279, 0.00100064, 0.0010004 ,\n",
       "        0.00100064, 0.00100064, 0.00200152, 0.00200176, 0.0010004 ,\n",
       "        0.00100112, 0.00100112, 0.00100064, 0.00200224, 0.00200176,\n",
       "        0.00100088, 0.00200129, 0.0010016 , 0.00200057, 0.0010004 ,\n",
       "        0.00100088, 0.00100088, 0.00099993, 0.00100064, 0.00100064,\n",
       "        0.00100112, 0.00200152, 0.00200272, 0.00200129, 0.002002  ,\n",
       "        0.00100088, 0.0010016 , 0.00200176, 0.00100088, 0.00200176,\n",
       "        0.0010016 , 0.00100088, 0.00100112, 0.00200129, 0.00200152,\n",
       "        0.0010004 , 0.00200057, 0.00100088, 0.00200152, 0.00200176,\n",
       "        0.00200152, 0.002002  , 0.00200129, 0.00100112, 0.00099754,\n",
       "        0.00100088, 0.00100279, 0.00100136, 0.00100088, 0.00100088,\n",
       "        0.002002  , 0.00100064, 0.00100088, 0.00200176, 0.00200176,\n",
       "        0.00100064, 0.00200176, 0.00100088, 0.00200129, 0.002002  ,\n",
       "        0.00100064, 0.00100088, 0.00100017, 0.00100112, 0.0010004 ,\n",
       "        0.0010004 , 0.00200176, 0.00200272, 0.00100112, 0.00200176,\n",
       "        0.00100088, 0.00100088, 0.0010004 , 0.00200152, 0.00100136,\n",
       "        0.00200152, 0.00200224, 0.00200129, 0.00100064, 0.00200176,\n",
       "        0.00100136, 0.00200105, 0.00200152, 0.00100064, 0.0010016 ,\n",
       "        0.00100064, 0.00100064, 0.00100088, 0.00100088, 0.00100088,\n",
       "        0.00100088, 0.00200176, 0.00200176, 0.00100112, 0.00200176,\n",
       "        0.0010004 , 0.00200152, 0.00200152, 0.00200129, 0.00100064,\n",
       "        0.00200176, 0.00100112, 0.002002  , 0.00100088, 0.00100088,\n",
       "        0.00200176, 0.00200176, 0.00200152, 0.00200152, 0.0010004 ,\n",
       "        0.00200176, 0.00100088, 0.00200129, 0.0010004 , 0.002002  ,\n",
       "        0.00200224, 0.00100136, 0.00200129, 0.00100088, 0.00100136,\n",
       "        0.00100112, 0.00200081, 0.00200176, 0.002002  , 0.0010004 ,\n",
       "        0.00100088, 0.0010004 , 0.00200248, 0.0010004 , 0.00200176,\n",
       "        0.00200152, 0.00200176, 0.00100064, 0.00100064, 0.00100088,\n",
       "        0.00200176, 0.00100064, 0.00100064, 0.00200129, 0.00200248,\n",
       "        0.00100064, 0.00100088, 0.00200176, 0.00100017, 0.00100088,\n",
       "        0.00100064, 0.002002  , 0.00200129, 0.00200129, 0.002002  ,\n",
       "        0.00200176, 0.00200152, 0.00200152, 0.00200129, 0.0018034 ,\n",
       "        0.00200176, 0.00200105, 0.00200224, 0.00200129, 0.00100136,\n",
       "        0.00100088, 0.00200152, 0.00100136, 0.00200152, 0.00100064,\n",
       "        0.00100088, 0.00100064, 0.00100017, 0.00200248, 0.00100064,\n",
       "        0.00100112, 0.0010004 , 0.00100064, 0.00100064, 0.002002  ,\n",
       "        0.00100088, 0.00200129, 0.00200081, 0.00200152, 0.00200248,\n",
       "        0.00100088, 0.00100112, 0.00100088, 0.00200129, 0.00100112,\n",
       "        0.00100088, 0.00100112, 0.00200105, 0.00100064, 0.00100064,\n",
       "        0.00100064, 0.00100088, 0.00200152, 0.00100064, 0.00200105,\n",
       "        0.00100088, 0.00100112, 0.00099993, 0.00200152, 0.00100064,\n",
       "        0.00100088, 0.00100112, 0.00200129, 0.00200152, 0.002002  ,\n",
       "        0.00100064, 0.00200152, 0.00200081, 0.00100112, 0.00100088,\n",
       "        0.0010016 , 0.00100088, 0.00200129, 0.00100088, 0.00200176,\n",
       "        0.00100064, 0.0010016 , 0.00100017, 0.00100088, 0.00100088,\n",
       "        0.00100064, 0.00100088, 0.00100088, 0.00200248, 0.00200152,\n",
       "        0.00100088, 0.00100112, 0.00200129, 0.00100064, 0.00100088,\n",
       "        0.00099993, 0.00100112, 0.00100112, 0.00100064, 0.00200176,\n",
       "        0.00100088, 0.00100112, 0.00100088, 0.00200152, 0.00200176,\n",
       "        0.00200176, 0.00200152, 0.00100064, 0.00100088, 0.00200176,\n",
       "        0.00200176, 0.00100088, 0.0010004 , 0.00100064, 0.00200176,\n",
       "        0.00200129, 0.00200105, 0.00100064, 0.0010016 , 0.0010004 ,\n",
       "        0.00200176, 0.00200129, 0.00200176, 0.00100064, 0.00099993,\n",
       "        0.0010004 , 0.00100088, 0.00100064, 0.00200176, 0.00100183,\n",
       "        0.00200129, 0.00100088, 0.00100064, 0.00100088, 0.00100088,\n",
       "        0.00200224, 0.00100112, 0.00100088, 0.00100017, 0.00200176,\n",
       "        0.0010004 , 0.00200176, 0.00200152, 0.002002  , 0.00100088,\n",
       "        0.00200129, 0.00100112, 0.00100088, 0.00200176, 0.00100064,\n",
       "        0.00200081, 0.00100064, 0.00200152, 0.00100064, 0.00100112,\n",
       "        0.00200152, 0.002002  , 0.00200152, 0.00200629, 0.00200081,\n",
       "        0.00200152, 0.00200248, 0.00100136, 0.00200272, 0.00200176,\n",
       "        0.00100112, 0.0010004 , 0.00200176, 0.00100088, 0.0010004 ,\n",
       "        0.00100112, 0.00200152, 0.00099945, 0.00200152, 0.00100064,\n",
       "        0.00200224, 0.00200176, 0.0010004 , 0.00100064, 0.00200152,\n",
       "        0.00200176, 0.00100136, 0.002002  , 0.00100064, 0.00200176,\n",
       "        0.00100112, 0.00100064, 0.00100088, 0.0010004 , 0.00100112,\n",
       "        0.00100064, 0.00100112, 0.00200152, 0.002002  , 0.00100064,\n",
       "        0.00100088, 0.00200152, 0.00100064, 0.00100064, 0.00100064,\n",
       "        0.00100064, 0.00200176, 0.00200343, 0.00100112, 0.00200152,\n",
       "        0.00100088, 0.00100088, 0.00100088, 0.00100088, 0.00200176,\n",
       "        0.00100064, 0.00100088, 0.00100064, 0.00100088, 0.00100088,\n",
       "        0.00100064, 0.00100088, 0.00200176, 0.00100088, 0.00100064,\n",
       "        0.00200176, 0.002002  , 0.00200152, 0.00200152, 0.00200176,\n",
       "        0.00100112, 0.00200152, 0.00100064, 0.0010004 , 0.00100088,\n",
       "        0.00200248, 0.00100088, 0.00200129, 0.002002  , 0.00200129,\n",
       "        0.00100088, 0.002002  , 0.00100112, 0.00100064, 0.00100088,\n",
       "        0.00100112, 0.00200176, 0.00100064, 0.002002  , 0.00200152,\n",
       "        0.00100064, 0.00200152, 0.00100064, 0.00200152, 0.00100088,\n",
       "        0.00100064, 0.00100112, 0.00100112, 0.00100136, 0.00200224,\n",
       "        0.00200081, 0.00200224, 0.00200129, 0.002002  , 0.00100088,\n",
       "        0.00200176, 0.00100064, 0.00200105, 0.0010016 , 0.00200152,\n",
       "        0.00200224, 0.00100088, 0.00100088, 0.00200248, 0.00100064,\n",
       "        0.0010004 , 0.00200129, 0.00100088, 0.00200224, 0.00200224,\n",
       "        0.00200152, 0.00100064, 0.0010004 , 0.00100112, 0.00100017,\n",
       "        0.00200152, 0.00100064, 0.00100088, 0.00100112, 0.00200176,\n",
       "        0.00200152, 0.00100064, 0.00200152, 0.00100064, 0.00100088,\n",
       "        0.00100088, 0.00100136, 0.00100088, 0.002002  , 0.00100088,\n",
       "        0.0010004 , 0.00200129, 0.00200105, 0.00100064, 0.00100136,\n",
       "        0.0010004 , 0.00200176, 0.00100088, 0.00100088, 0.0010004 ,\n",
       "        0.00200176, 0.00100207, 0.00200224, 0.00200129, 0.00200081,\n",
       "        0.00200176, 0.00100088, 0.002002  , 0.00200176, 0.00100064,\n",
       "        0.00100112, 0.00200057, 0.00200176, 0.00100064, 0.00100064,\n",
       "        0.002002  , 0.00200105, 0.00100088, 0.00100064, 0.0010016 ,\n",
       "        0.00100064, 0.00200176, 0.00100064, 0.0010016 , 0.00200009,\n",
       "        0.00200176, 0.0010016 , 0.00200176, 0.00100112, 0.00100088,\n",
       "        0.00200248, 0.00100064, 0.00200176, 0.00100064, 0.00200152,\n",
       "        0.0010004 , 0.0010004 , 0.00100088, 0.00200176, 0.00100088,\n",
       "        0.00200176, 0.00200152, 0.00200176, 0.00200129, 0.00100088,\n",
       "        0.00200176, 0.00200224, 0.00100088, 0.00100088, 0.00200224,\n",
       "        0.00100064, 0.002002  , 0.00100112, 0.00100064, 0.00100064,\n",
       "        0.0010016 , 0.00200176, 0.00200248, 0.00200224, 0.00100088,\n",
       "        0.00100064, 0.00100112, 0.00200248, 0.00200152, 0.00200176,\n",
       "        0.00200176, 0.00100017, 0.00200152, 0.00100088, 0.00100064,\n",
       "        0.00200129, 0.00100112, 0.00200176, 0.00100112, 0.00200152,\n",
       "        0.00200129, 0.00100088, 0.00200152, 0.0010016 , 0.0010004 ,\n",
       "        0.00100112, 0.00100088, 0.00100088, 0.0010004 , 0.00100088,\n",
       "        0.00100088, 0.00100088, 0.00100064, 0.00200152, 0.00100088,\n",
       "        0.0010004 , 0.0010004 , 0.002002  , 0.00100064, 0.00100112,\n",
       "        0.00200248, 0.00200129, 0.00200176, 0.00200129, 0.00100088,\n",
       "        0.00100017, 0.002002  , 0.00100112, 0.00100064, 0.00200224,\n",
       "        0.00200152, 0.00100088, 0.00100136, 0.00200152, 0.00100064,\n",
       "        0.00200081, 0.00100064, 0.00200129, 0.00100136, 0.00200248,\n",
       "        0.0010004 , 0.00200152, 0.00100088, 0.002002  , 0.00100136,\n",
       "        0.00200152, 0.00100064, 0.00100064, 0.00100112, 0.00100064,\n",
       "        0.00200176, 0.00100064, 0.00100088, 0.00200152, 0.00200129,\n",
       "        0.00100017, 0.00100064, 0.00100088, 0.00200129, 0.00200152,\n",
       "        0.00100088, 0.00100064, 0.00100112, 0.002002  , 0.00100088,\n",
       "        0.00200152, 0.00100088, 0.00200129, 0.00100112, 0.00200129,\n",
       "        0.00100064, 0.00200129, 0.002002  , 0.00200081, 0.00200152,\n",
       "        0.00100088, 0.00100088, 0.00100136, 0.00200176, 0.00100064,\n",
       "        0.00200176, 0.00100088, 0.00200152, 0.00100112, 0.00100088,\n",
       "        0.00200057, 0.00200224, 0.00200176, 0.002002  , 0.002002  ,\n",
       "        0.00100064, 0.00100112, 0.00100088, 0.00100017, 0.00100088,\n",
       "        0.00200152, 0.00100088, 0.00100064, 0.00100064, 0.00100088,\n",
       "        0.00200176, 0.00200129, 0.00100112, 0.00200152, 0.00100064,\n",
       "        0.0010004 , 0.00100088, 0.00100136, 0.00100088, 0.00200176,\n",
       "        0.00200152, 0.00100088, 0.0010004 , 0.00200152, 0.0010016 ,\n",
       "        0.00200129, 0.00100064, 0.00200176, 0.00200129, 0.0010004 ,\n",
       "        0.00100088, 0.002002  , 0.00200224, 0.002002  , 0.00100088,\n",
       "        0.00200248, 0.00200129, 0.00199962, 0.00200105, 0.00100088,\n",
       "        0.00100112, 0.00100183, 0.00200129, 0.00200176, 0.00200176,\n",
       "        0.00100064, 0.00100064, 0.0010004 , 0.002002  , 0.00200176,\n",
       "        0.00200176, 0.00100088, 0.00200129, 0.00100088, 0.00100112,\n",
       "        0.00100064, 0.00100088, 0.0010004 , 0.00100112, 0.00100112,\n",
       "        0.00100112, 0.00200319, 0.00200152, 0.00200176, 0.00100088,\n",
       "        0.00200176, 0.00100064, 0.00100136, 0.00100112, 0.0010004 ,\n",
       "        0.00200152, 0.0010004 , 0.00200129, 0.00100064, 0.00200152,\n",
       "        0.00200152, 0.00200129, 0.00200272, 0.00200152, 0.00200224,\n",
       "        0.00100064, 0.00200152, 0.00100064, 0.00100088, 0.00100064,\n",
       "        0.00100088, 0.00100064, 0.00200176, 0.00200152, 0.00100088,\n",
       "        0.002002  , 0.00100064, 0.00200176, 0.00100064, 0.00400448,\n",
       "        0.0010004 , 0.00100064, 0.00100088, 0.00200176, 0.00100088,\n",
       "        0.00100064, 0.0010004 , 0.00100064, 0.00100064, 0.00200272,\n",
       "        0.00200295, 0.00100088, 0.00100088, 0.0010004 , 0.002002  ,\n",
       "        0.00100088, 0.00200129, 0.00200152, 0.00200176, 0.00200319,\n",
       "        0.00200176, 0.00200152, 0.00100064, 0.00100064, 0.00200176,\n",
       "        0.00100112, 0.00200224, 0.00100136]),\n",
       " 'test_accuracy': array([1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "        1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
       "        1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
       "        1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
       "        0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
       "        0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "        0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
       "        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "        0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
       "        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
       "        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
       "        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
       "        1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
       "        1., 0., 1.])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "cv_results = cross_validate(mlp, X, y, cv=LeaveOneOut(), scoring=[\"accuracy\"])\n",
    "# cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7135416666666666"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results['test_accuracy'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
